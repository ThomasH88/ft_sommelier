{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "\n",
    "pd.set_option('display.max_rows', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_wine_df = pd.read_csv(\"resources/winequality-red.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lenght =\", len(red_wine_df))\n",
    "red_wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.1 Exploring the green reds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Plot scatter matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_matrix(data, good_threshold, bad_threshold, rows=12, cols=12, save_plot=False, name=None):\n",
    "    fig, axmat = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "    for axrow in axmat:\n",
    "        for ax in axrow:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    h_list = data.columns.values\n",
    "    for i in range(len(h_list)):\n",
    "        for j in range(len(h_list)):\n",
    "            plt.sca(axmat[i][j])\n",
    "            if (i == j):\n",
    "                plt.text(0.5, 0.5, h_list[j].replace(' ', '\\n'), fontsize=16, ha='center', va='center')\n",
    "            else:\n",
    "                plt.scatter(data[h_list[j]][data['quality'] > good_threshold],\n",
    "                            data[h_list[i]][data['quality'] > good_threshold], s=3, c='g')\n",
    "                plt.scatter(data[h_list[j]][data['quality'] < bad_threshold],\n",
    "                            data[h_list[i]][data['quality'] < bad_threshold], s=3, c='m')\n",
    "    if (save_plot):\n",
    "        if (name == None):\n",
    "            plt.savefig('Plt.png')\n",
    "        else:\n",
    "            plt.savefig(\"{}.png\".format(name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_matrix(red_wine_df, 6, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Which factors do you think will be most useful for distinguishing high vs low quality wines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first glance I see that pH and alcohol have a clearer division when it comes to quality. We can plot a decision boundary that will divide the data perfectly using these 2 features. This is the case when comparing high quality wines (8 or higher) vs low quality wines (3 or lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.2 Learning to perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)&b) Perceptron implementation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_labels(data, good_thres=7, bad_thres=4, feature_list=[\"pH\", \"alcohol\"], labels=(1, 0)):\n",
    "    tmp = data[feature_list + [\"quality\"]][(\n",
    "        data['quality'] > good_thres) | (data['quality'] < bad_thres)]\n",
    "    tmp['quality'].where(tmp['quality'] < bad_thres, labels[0], inplace=True)\n",
    "    tmp['quality'].where(tmp['quality'] == 1, labels[1], inplace=True)\n",
    "    features = tmp[feature_list].values\n",
    "    labels = tmp['quality'].values\n",
    "    return (features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = pd.DataFrame([random.uniform(-1, 1)])\n",
    "        for i in range(input_size - 1):\n",
    "            self.weights = pd.concat([self.weights, pd.DataFrame([random.uniform(-1, 1)])])\n",
    "        self.weights = self.weights.values.flatten()\n",
    "        self.bias = pd.DataFrame([random.uniform(-1, 1)]).values[0]\n",
    "    def heaviside_step_fn(self, nb):\n",
    "        if (nb >= 0):\n",
    "            return (1)\n",
    "        else:\n",
    "            return (0)\n",
    "    def forward_pass(self, X):\n",
    "        return (self.heaviside_step_fn(sum(self.weights * X) + self.bias))\n",
    "    def evaluate(self, features, labels):\n",
    "        errors = 0\n",
    "        for X, y in zip(features, labels):\n",
    "            output = self.forward_pass(X)\n",
    "            if (output != y):\n",
    "                errors += 1\n",
    "        return (errors)\n",
    "    def update_weights(self, output, X, y, l_rate):\n",
    "        error = y - output\n",
    "        if (error != 0):\n",
    "            self.bias += l_rate * error\n",
    "            self.weights += error * l_rate * X\n",
    "    def train(self, data, l_rate, epochs, thres=(7, 4), feature_list=[\"pH\", \"alcohol\"]):\n",
    "        features, labels = select_features_labels(data, thres[0], thres[1], feature_list=feature_list)\n",
    "        if (epochs == 0):\n",
    "            epochs = 20000\n",
    "        perf = []\n",
    "        perf.append((0, self.evaluate(features, labels), copy.copy(self.weights), copy.copy(self.bias)))\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            for X, y in zip(features, labels):\n",
    "                output = self.forward_pass(X)\n",
    "                self.update_weights(output, X, y, l_rate)\n",
    "            errors = self.evaluate(features, labels)\n",
    "            perf.append((epoch, errors, copy.copy(self.weights), copy.copy(self.bias)))\n",
    "            if (errors == 0):\n",
    "                break\n",
    "        self.perf = perf\n",
    "        return (perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Plot perceptron performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perceptron_performance(wine_data, performance, good_threshold,\n",
    "                                bad_threshold, feature_list=[\"pH\", \"alcohol\"], epoch=-1):\n",
    "    fig, axvec = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    if (epoch >= 0):\n",
    "        performance = performance[: epoch + 1]\n",
    "    else:\n",
    "        epoch = performance[-1][0]\n",
    "    df = pd.DataFrame(performance)\n",
    "    \n",
    "    plt.sca(axvec[0])\n",
    "    plt.plot(df[0], df[1], color=\"navy\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('classification errors')\n",
    "    plt.title('Error as a function of epoch')\n",
    "    \n",
    "    plt.sca(axvec[1])\n",
    "    epsilon = wine_data[feature_list[0]].min() / 20\n",
    "    epsilon_2 = wine_data[feature_list[1]].min() / 5\n",
    "    xmin = wine_data[feature_list[0]].min() - epsilon\n",
    "    xmax = wine_data[feature_list[0]].max() + epsilon\n",
    "    ymin = wine_data[feature_list[1]].min() - epsilon_2\n",
    "    ymax = wine_data[feature_list[1]].max() + epsilon_2\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([xmin, xmax])\n",
    "    axes.set_ylim([ymin, ymax])\n",
    "    good_label = 'good wines (> {} score)'.format(good_threshold)\n",
    "    bad_label = 'bad wines (< {} score)'.format(bad_threshold)\n",
    "    good = plt.scatter(wine_data[feature_list[0]][wine_data['quality'] > good_threshold],\n",
    "                wine_data[feature_list[1]][wine_data['quality'] > good_threshold], s=15, c='g', label=good_label)\n",
    "    bad = plt.scatter(wine_data[feature_list[0]][wine_data['quality'] < bad_threshold],\n",
    "                wine_data[feature_list[1]][wine_data['quality'] < bad_threshold], s=15, c='m', label=bad_label)\n",
    "    x_plot = [xmin, xmax]\n",
    "    w = performance[-1][2]\n",
    "    b = performance[-1][3]\n",
    "    print(\"learned weights\", w)\n",
    "    print(\"learned bias\", b)\n",
    "    y = [0, 0]\n",
    "    y[0] = (-1 / w[1]) * (w[0] * x_plot[0] + b[0])\n",
    "    y[1] = (-1 / w[1]) * (w[0] * x_plot[1] + b[0])\n",
    "    d_boundary = plt.plot(x_plot, y, label=\"decision boundary\", linestyle='dashed', color=\"navy\")\n",
    "    plt.fill_between(x_plot, ymin, y, alpha=0.2, color=\"m\")\n",
    "    plt.fill_between(x_plot, y, ymax, alpha=0.2, color=\"g\")\n",
    "    \n",
    "    plt.xlabel(feature_list[0])\n",
    "    plt.ylabel(feature_list[1])\n",
    "    plt.title('Decision boundary on epoch {}'.format(epoch))\n",
    "    plt.legend(loc=(1.01, 0.82))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_perceptron = Perceptron(2)\n",
    "performance = my_perceptron.train(red_wine_df, l_rate=0.9, epochs=0)\n",
    "plot_perceptron_performance(red_wine_df, performance, 7, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Feature scaling for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(wine_data):\n",
    "    quality = wine_data[\"quality\"]\n",
    "    norm_wine_data = wine_data.drop(labels=\"quality\", axis='columns')\n",
    "    norm_wine_data = (norm_wine_data - norm_wine_data.mean()) / (norm_wine_data.max() - norm_wine_data.min())\n",
    "    norm_wine_data[\"quality\"] = quality\n",
    "    return (norm_wine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_perceptron = Perceptron(2)\n",
    "norm_perf = my_perceptron.train(normalize_data(red_wine_df), l_rate=0.9, epochs=0)\n",
    "plot_perceptron_performance(normalize_data(red_wine_df), norm_perf, 7, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.3 My fair ADALINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Perceptrons don't work when the data is not linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_perceptron = Perceptron(2)\n",
    "performance = my_perceptron.train(red_wine_df, l_rate=0.9, epochs=0, thres=(6, 4))\n",
    "plot_perceptron_performance(red_wine_df, performance, 6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)&c) Implement an ADALINE with gradient descent and a training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "        self.weights = pd.DataFrame([random.uniform(-1, 1)])\n",
    "        for i in range(input_size - 1):\n",
    "            self.weights = pd.concat([self.weights, pd.DataFrame([random.uniform(-1, 1)])])\n",
    "        self.weights = self.weights.values.flatten()\n",
    "        self.bias = pd.DataFrame([random.uniform(-1, 1)]).values[0]\n",
    "    def heaviside_step_fn(self, nb):\n",
    "        if (nb >= 0):\n",
    "            return (1)\n",
    "        else:\n",
    "            return (-1)\n",
    "    def forward_pass(self, X):\n",
    "        return (sum(self.weights * X) + self.bias)\n",
    "    def evaluate(self, features, labels):\n",
    "        errors = 0\n",
    "        for X, y in zip(features, labels):\n",
    "            output = self.heaviside_step_fn(self.forward_pass(X))\n",
    "            if (output != y):\n",
    "                errors += 1\n",
    "        return (errors)\n",
    "    def update_weights(self, w_gradient, b_gradient, l_rate):\n",
    "        self.bias += b_gradient * l_rate\n",
    "        self.weights += w_gradient * l_rate\n",
    "    def next_batch(self, features, labels, batch_size):\n",
    "        for i in range(0, len(labels), batch_size):\n",
    "            yield features[:][i:i + batch_size], labels[i:i + batch_size]\n",
    "    def batch_processing(self, features, labels, l_rate, batch_size):\n",
    "        generator = self.next_batch(features, labels, batch_size)\n",
    "        for batch_X, batch_y in generator:\n",
    "            w_gradient = pd.DataFrame([0.] * self.input_size).values.flatten()\n",
    "            b_gradient = 0.\n",
    "            for X, y in zip(batch_X, batch_y):\n",
    "                gradient = (y - self.forward_pass(X))\n",
    "                w_gradient += gradient * X\n",
    "                b_gradient += gradient\n",
    "            self.update_weights(w_gradient, b_gradient, l_rate)\n",
    "    def train(self, data, l_rate, epochs, thres=(7, 4), training=\"online\",\n",
    "                feature_list=[\"pH\", \"alcohol\"], nb=38, labels=(1, -1)):\n",
    "        if (training == \"online\"):\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = 32\n",
    "        features, labels = select_features_labels(data, thres[0], thres[1],\n",
    "                                                  labels=labels, feature_list=feature_list)\n",
    "        if (epochs == 0):\n",
    "            epochs = 200\n",
    "        perf = []\n",
    "        perf.append((0, self.evaluate(features, labels), copy.copy(self.weights), copy.copy(self.bias)))\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            errors = self.evaluate(features, labels)\n",
    "            perf.append((epoch, errors, copy.copy(self.weights), copy.copy(self.bias)))\n",
    "            self.batch_processing(features, labels, l_rate, batch_size)\n",
    "            if (errors <= nb):\n",
    "                break\n",
    "        self.perf = perf\n",
    "        return (perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_adaline = Adaline(2)\n",
    "performance = my_adaline.train(red_wine_df, l_rate=0.00099, epochs=0, thres=(6, 5), training=\"online\")\n",
    "plot_perceptron_performance(red_wine_df, performance, 6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_adaline = Adaline(2)\n",
    "performance = my_adaline.train(red_wine_df, l_rate=0.0003, epochs=0, thres=(6, 5), training=\"batch\")\n",
    "plot_perceptron_performance(red_wine_df, performance, 6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_adaline = Adaline(2)\n",
    "norm_perf = my_adaline.train(normalize_data(red_wine_df), l_rate=0.025, epochs=0, thres=(6, 5), training=\"batch\")\n",
    "plot_perceptron_performance(normalize_data(red_wine_df), norm_perf, 6, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.4 Advanced wine sampling and resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Holdout method to partition data in training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(wine_data, validation_split):\n",
    "    wine_data = wine_data.sample(frac=1).reset_index(drop=True) # shuffle data\n",
    "    size = len(wine_data)\n",
    "    validation_size = round(size * validation_split)\n",
    "    training_size = size - validation_size\n",
    "    return (wine_data.head(training_size), wine_data.tail(validation_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) k-fold cross-validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_validation(wine_data, k, shuffle=True):\n",
    "    if (shuffle == True):\n",
    "        wine_data = wine_data.sample(frac=1).reset_index(drop=True)\n",
    "    k_val_sizes = [(len(wine_data) // k) + 1 if (i < len(wine_data) % k) else (len(wine_data) // k) for i in range(k)]\n",
    "    k_tra_sizes = [len(wine_data) - j for j in k_val_sizes]\n",
    "    k_folds = []\n",
    "    i = 0\n",
    "    for val_size in k_val_sizes:\n",
    "        k_folds.append(wine_data[i:i + val_size])\n",
    "        i += val_size\n",
    "    k_tuples = []\n",
    "    for i in range(k):\n",
    "        training = pd.DataFrame()\n",
    "        for j in range(k):\n",
    "            if (i != j):\n",
    "                training = pd.concat((training, k_folds[j]))\n",
    "        k_tuples.append((training, k_folds[i]))\n",
    "    return (k_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Effects on changing learning rate and epochs: k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaline_k_fold_validation(wine_data, l_rate, k, shuffle=True,\n",
    "                              epochs=10, thres=(6, 5)):\n",
    "    if (epochs == 0):\n",
    "        return(print(\"epochs can't be 0\"))\n",
    "    tmp = wine_data[['pH', 'alcohol', 'quality']][(\n",
    "        wine_data['quality'] > thres[0]) | (wine_data['quality'] < thres[1])]\n",
    "    tmp['quality'].where(tmp['quality'] < thres[1], 1, inplace=True) # Replace above good_quality_nb with 1\n",
    "    tmp['quality'].where(tmp['quality'] == 1, 0, inplace=True) # and below bad_quality_nb 0\n",
    "    k_folds = k_fold_validation(tmp, k, shuffle=shuffle)\n",
    "    my_adaline = Adaline(2)\n",
    "    \n",
    "    errors = 0\n",
    "    val_errors = 0\n",
    "    for fold in k_folds:\n",
    "        features, labels = select_features_labels(fold[0], thres[0], thres[1])\n",
    "        val_features, val_labels = select_features_labels(fold[1], thres[0], thres[1])\n",
    "        errors += my_adaline.evaluate(features, labels)\n",
    "        val_errors += my_adaline.evaluate(val_features, val_labels)\n",
    "    errors /= len(k_folds)\n",
    "    val_errors /= len(k_folds)\n",
    "    print(\"epoch 0: average training     errors: {:<4}/{:<4} => {:<2}%\".format(errors, len(labels), round(errors / len(labels) * 100, 2)))\n",
    "    print(\"         average validation   errors: {:<4}/{:<4} => {:<2}%\".format(val_errors, len(val_labels), round(val_errors / len(val_labels) * 100, 2)))\n",
    "    print(\"weights learned\", my_adaline.weights)\n",
    "    print(\"bias learned\", my_adaline.bias)\n",
    "    print()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        errors = 0\n",
    "        val_errors = 0\n",
    "        for fold in k_folds:\n",
    "            features, labels = select_features_labels(fold[0], thres[0], thres[1])\n",
    "            val_features, val_labels = select_features_labels(fold[1], thres[0], thres[1])\n",
    "            my_adaline.train(fold[0], l_rate=l_rate, epochs=1, thres=thres)\n",
    "            e = my_adaline.evaluate(features, labels)\n",
    "            val_e = my_adaline.evaluate(val_features, val_labels)\n",
    "            errors += e\n",
    "            val_errors += val_e\n",
    "        errors /= len(k_folds)\n",
    "        val_errors /= len(k_folds)\n",
    "        print(\"epoch {}: average training     errors: {:<4}/{:<4} => {:<2}%\".format(epoch + 1, errors, len(labels), round(errors / len(labels) * 100, 2)))\n",
    "        print(\"         average validation   errors: {:<4}/{:<4} => {:<2}%\".format(val_errors, len(val_labels), round(val_errors / len(val_labels) * 100, 2)))\n",
    "        print(\"weights learned\", my_adaline.weights)\n",
    "        print(\"bias learned\", my_adaline.bias)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning rate too high, it diverges instead of converging on a local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaline_k_fold_validation(red_wine_df, l_rate=0.1, k=10, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning rate too small and few epochs it does not improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaline_k_fold_validation(red_wine_df, l_rate=0.0000001, k=10, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaline_k_fold_validation(red_wine_df, l_rate=0.003, k=10, epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If the learning rate is set well there is no need for many epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaline_k_fold_validation(red_wine_df, l_rate=0.001, k=10, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.5 Adventures in the Nth dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Trying more and different chemical factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\"pH\", \"alcohol\", \"sulphates\"]\n",
    "my_adaline = Adaline(3)\n",
    "perf = my_adaline.train(red_wine_df, l_rate=0.0003, epochs=2000,\n",
    "                        thres=(6, 5), training=\"batch\", feature_list=feature_list, nb=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in my_adaline.perf:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\"pH\", \"alcohol\", \"sulphates\", \"chlorides\"]\n",
    "my_adaline = Adaline(4)\n",
    "perf = my_adaline.train(red_wine_df, l_rate=0.0003, epochs=2000,\n",
    "                        thres=(6, 5), training=\"batch\", feature_list=feature_list, nb=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in my_adaline.perf:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\"chlorides\", \"sulphates\"]\n",
    "my_adaline = Adaline(2)\n",
    "perf = my_adaline.train(red_wine_df, l_rate=0.03, epochs=2000,\n",
    "                        thres=(6, 5), training=\"batch\", feature_list=feature_list, nb=40)\n",
    "plot_perceptron_performance(red_wine_df, perf, 6, 5, feature_list=feature_list, epoch=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Multiple dimensions decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of 3 dimensions the decision boundary will be a plane that separates the data in a 3d space. In 4 or higher dimensions it will be a hyperplane 1 dimension lower than the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.6 Marvin's rebuttal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Pan-Galactic Gargle Blaset dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galactic_df = pd.read_csv(\"resources/Pan Galactic Gargle Blaster.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galactic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galactic_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_matrix(galactic_df, 5, 5, rows=3, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(galactic_data):\n",
    "    galactic_data.loc[(galactic_data[\"wonderflonium\"] > 3.756) &\n",
    "                      (galactic_data[\"wonderflonium\"] < 4.689), \"wonderflonium\"] /= 2\n",
    "    galactic_data.loc[(galactic_data[\"fallian marsh gas\"] > 3.770) & \n",
    "                      (galactic_data[\"fallian marsh gas\"] < 4.689), \"wonderflonium\"] /= 2\n",
    "    galactic_data.loc[(galactic_data[\"wonderflonium\"] > 1.2), \"wonderflonium\"] += 10\n",
    "    galactic_data.loc[(galactic_data[\"wonderflonium\"] < 1.2), \"wonderflonium\"] += 5\n",
    "    return (galactic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galactic_df = pd.read_csv(\"resources/Pan Galactic Gargle Blaster.csv\", sep=\";\")\n",
    "galactic_data = transform_data(galactic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_matrix(galactic_data, 5, 5, rows=3, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galactic_df = pd.read_csv(\"resources/Pan Galactic Gargle Blaster.csv\", sep=\";\")\n",
    "galactic_data = transform_data(galactic_df)\n",
    "feature_list = [\"wonderflonium\", \"fallian marsh gas\"]\n",
    "my_adaline = Adaline(2)\n",
    "perf = my_adaline.train(galactic_data, l_rate=0.0005, epochs=100,\n",
    "                        thres=(5, 5), training=\"batch\", feature_list=feature_list, nb=0)\n",
    "plot_perceptron_performance(galactic_data, perf, 5, 5, feature_list=feature_list, epoch=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

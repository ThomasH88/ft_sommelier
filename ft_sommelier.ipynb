{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FT_SOMMELIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_wine_df = pd.read_csv(\"resources/winequality-red.csv\", sep=';')\n",
    "white_wine_df = pd.read_csv(\"resources/winequality-white.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(red_wine_df))\n",
    "red_wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.1 Exploring the green reds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Plot scatter matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_matrix(wine_data, good_threshold, bad_threshold, rows=12, cols=12, save_plot=False, name=None):\n",
    "    fig, axmat = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "    for axrow in axmat:\n",
    "        for ax in axrow:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    h_list = wine_data.columns.values\n",
    "    for i in range(len(h_list)):\n",
    "        for j in range(len(h_list)):\n",
    "            plt.sca(axmat[i][j])\n",
    "            if (i == j):\n",
    "                plt.text(0.5, 0.5, h_list[j].replace(' ', '\\n'), fontsize=16, ha='center', va='center')\n",
    "            else:\n",
    "                plt.scatter(wine_data[h_list[j]][wine_data['quality'] > good_threshold],\n",
    "                            wine_data[h_list[i]][wine_data['quality'] > good_threshold], s=3, c='g')\n",
    "                plt.scatter(wine_data[h_list[j]][wine_data['quality'] < bad_threshold],\n",
    "                            wine_data[h_list[i]][wine_data['quality'] < bad_threshold], s=3, c='m')\n",
    "    if (save_plot):\n",
    "        if (name == None):\n",
    "            plt.savefig('Plt.png') # bbox_inches='tight' to remove white space around\n",
    "        else:\n",
    "            plt.savefig(\"{}.png\".format(name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_matrix(red_wine_df, 6, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Which factors do you think will be most useful for distinguishing high vs low quality wines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first glance I see that sulphates and alcohol have a clearer division when it comes to quality.\n",
    "We can see in the figure that in most cases a higher alcohol quantity and a higher level of sulphates results\n",
    "in a beter quality wine. This is the case when comparing high quality wines (8 or higher) vs low quality wines\n",
    "(3 or lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.2 Learning to perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) & b) Perceptron implementation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heaviside_step_fn(nb):\n",
    "    if (nb < 0):\n",
    "        return (0)\n",
    "    elif (nb >= 0):\n",
    "        return (1)\n",
    "\n",
    "def dot_prod(v1, v2):\n",
    "    return (sum([x * y for x, y in zip(v1, v2)]))\n",
    "\n",
    "def vec_add(v1, v2):\n",
    "    return ([round(x + y, 2) for x, y in zip(v1, v2)])\n",
    "    \n",
    "def vec_sub(v1, v2):\n",
    "    return ([round(x - y, 2) for x, y in zip(v1, v2)])\n",
    "\n",
    "def vec_scale(v, scalar):\n",
    "    return ([i * scalar for i in v])\n",
    "\n",
    "def select_features_labels(wine_data, good_quality_nb, bad_quality_nb, features_list=[\"pH\", \"alcohol\"]):\n",
    "    tmp = wine_data[features_list + [\"quality\"]][(\n",
    "        wine_data['quality'] > good_quality_nb) | (wine_data['quality'] < bad_quality_nb)]\n",
    "    tmp['quality'].where(tmp['quality'] < bad_quality_nb, 1, inplace=True) # Replace good_quality_nb and above with 1\n",
    "    tmp['quality'].where(tmp['quality'] == 1, 0, inplace=True) # and bad quality and below with 0\n",
    "    features = tmp[features_list]\n",
    "    labels = tmp[['quality']]\n",
    "    features = [list(features[item]) for item in features]\n",
    "    labels = [list(labels[item]) for item in labels]\n",
    "    labels = labels[0]\n",
    "    return (features, labels) # returns a list of lists containing the values of each feature\n",
    "\n",
    "def update_weights(w, x, y, l_rate):\n",
    "    update = False\n",
    "    if (heaviside_step_fn(dot_prod(w, x)) == 1 and y == 0):\n",
    "        update = True\n",
    "        w = vec_sub(w, vec_scale(x, l_rate)) # w = w - lr * x\n",
    "    elif (heaviside_step_fn(dot_prod(w, x)) == 0 and y == 1):\n",
    "        update = True\n",
    "        w = vec_add(w, vec_scale(x, l_rate)) # w = w + lr * x\n",
    "    return (w, update)\n",
    "    \n",
    "def perceptron_trainer(wine_data, l_rate, epochs):\n",
    "    features, labels = select_features_labels(wine_data, 7, 4)\n",
    "    train_until_convergence = False\n",
    "    epoch_count = 0\n",
    "    info = []\n",
    "    if (epochs == 0):\n",
    "        train_until_convergence = True # train until convergence if epochs is 0\n",
    "        epochs = 1 # epoch_count < epochs so it enters the loop\n",
    "    converged = False\n",
    "    w = [0, 0, 0] # w[0] is the bias\n",
    "    for i in range(3):\n",
    "        w[i] = round(random.uniform(-1, 1), 2) # init random weights between -1 and 1\n",
    "    while (not converged and epoch_count < epochs):\n",
    "        converged = True\n",
    "        errors_at_epoch = 0\n",
    "        for x1, x2, y in zip(features[0], features[1], labels):\n",
    "            x = [1, x1, x2]\n",
    "            w, update = update_weights(w, x, y, l_rate)\n",
    "            if (update == True): # if there was no update in the whole dataset that means our model has converged\n",
    "                converged = False\n",
    "                errors_at_epoch += 1\n",
    "        info.append((epoch_count, errors_at_epoch, w))\n",
    "        epoch_count += 1\n",
    "        if (train_until_convergence == True):\n",
    "            epochs = epoch_count + 1 # we don't want to stop training until convergence\n",
    "    return (info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1998)\n",
    "performance = perceptron_trainer(red_wine_df, l_rate=0.98, epochs=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Plot perceptron performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perceptron_performance(wine_data, performance, good_threshold, bad_threshold, feature_list=[\"pH\", \"alcohol\"], epoch=-1):\n",
    "    fig, axvec = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    if (epoch >= 0):\n",
    "        performance = performance[: epoch + 1]\n",
    "    else:\n",
    "        epoch = performance[-1][0]\n",
    "    df = pd.DataFrame(performance)\n",
    "    \n",
    "    plt.sca(axvec[0])\n",
    "    plt.plot(df[0], df[1], color=\"navy\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('classification errors')\n",
    "    plt.title('Error as a function of epoch')\n",
    "    \n",
    "    plt.sca(axvec[1])\n",
    "    epsilon = wine_data[feature_list[0]].min() / 20\n",
    "    epsilon_2 = wine_data[feature_list[1]].min() / 5\n",
    "    xmin = wine_data[feature_list[0]].min() - epsilon\n",
    "    xmax = wine_data[feature_list[0]].max() + epsilon\n",
    "    ymin = wine_data[feature_list[1]].min() - epsilon_2\n",
    "    ymax = wine_data[feature_list[1]].max() + epsilon_2\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([xmin, xmax])\n",
    "    axes.set_ylim([ymin, ymax])\n",
    "    good_label = 'good wines (> {} score)'.format(good_threshold)\n",
    "    bad_label = 'bad wines (< {} score)'.format(bad_threshold)\n",
    "    good = plt.scatter(wine_data[feature_list[0]][wine_data['quality'] > good_threshold],\n",
    "                wine_data[feature_list[1]][wine_data['quality'] > good_threshold], s=15, c='g', label=good_label)\n",
    "    bad = plt.scatter(wine_data[feature_list[0]][wine_data['quality'] < bad_threshold],\n",
    "                wine_data[feature_list[1]][wine_data['quality'] < bad_threshold], s=15, c='m', label=bad_label)\n",
    "    x_plot = [xmin, xmax]\n",
    "    w = performance[-1][2]\n",
    "    print(\"learned weights\", w)\n",
    "    y = [0, 0]\n",
    "    y[0] = (-1 / w[2]) * (w[1] * x_plot[0] + w[0])\n",
    "    y[1] = (-1 / w[2]) * (w[1] * x_plot[1] + w[0])\n",
    "    d_boundary = plt.plot(x_plot, y, label=\"decision boundary\", linestyle='dashed', color=\"navy\")\n",
    "    plt.fill_between(x_plot, ymin, y, alpha=0.2, color=\"m\")\n",
    "    plt.fill_between(x_plot, y, ymax, alpha=0.2, color=\"g\")\n",
    "    \n",
    "    plt.xlabel(feature_list[0])\n",
    "    plt.ylabel(feature_list[1])\n",
    "    plt.title('Decision boundary on epoch {}'.format(epoch))\n",
    "    plt.legend(loc=(1.01, 0.82))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perceptron_performance(red_wine_df, performance, 7, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Feature scaling for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(wine_data):\n",
    "    quality = wine_data[\"quality\"]\n",
    "    norm_wine_data = wine_data.drop(labels=\"quality\", axis='columns')\n",
    "    norm_wine_data = (norm_wine_data - norm_wine_data.mean()) / (norm_wine_data.max() - norm_wine_data.min())\n",
    "    norm_wine_data[\"quality\"] = quality\n",
    "    return (norm_wine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1998)\n",
    "norm_red_wine_df = normalize_data(red_wine_df)\n",
    "norm_perf = perceptron_trainer(norm_red_wine_df, l_rate=0.98, epochs=0)\n",
    "plot_perceptron_performance(norm_red_wine_df, norm_perf, 7, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.3 My fair ADALINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)&b)&c) Implement an ADALINE with gradient descent and a training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline:\n",
    "    def __init__(self):\n",
    "        self.weights = [0, 0, 0]\n",
    "        for i in range(3):\n",
    "            self.weights[i] = round(random.uniform(-1, 1), 2) # w[0] is the bias\n",
    "    def test(self, features, labels):\n",
    "        error_count = 0\n",
    "        for x1, x2, y in zip(features[0], features[1], labels):\n",
    "            x = [1, x1, x2]\n",
    "            if (y == 0):\n",
    "                y = -1\n",
    "            if (dot_prod(self.weights, x) >= 0):\n",
    "                output = 1\n",
    "            else:\n",
    "                output = -1\n",
    "            if (y != output):\n",
    "                error_count += 1\n",
    "        return (error_count)\n",
    "    def next_batch(self, features, labels, batch_size):\n",
    "        for i in range(0, len(labels), batch_size):\n",
    "            yield features[0][i:i + batch_size], features[1][i:i + batch_size], labels[i:i + batch_size]\n",
    "    def batch_processing(self, features, labels, l_rate, batch_size):\n",
    "        generator = self.next_batch(features, labels, batch_size)\n",
    "        for batch_x1, batch_x2, batch_y in generator:\n",
    "            batch_error = 0\n",
    "            batch_gradient = [0, 0, 0]\n",
    "            for x1, x2, y in zip(batch_x1, batch_x2, batch_y):\n",
    "                x = [1, x1, x2]\n",
    "                if (y == 0):\n",
    "                    y = -1\n",
    "                error = y - dot_prod(self.weights, x)\n",
    "                gradient = vec_scale(v=x, scalar=2 * l_rate * error)\n",
    "                batch_error += error\n",
    "                batch_gradient = vec_add(batch_gradient, gradient)\n",
    "            batch_error /= len(batch_x1)\n",
    "            self.weights = vec_add(self.weights, batch_gradient)\n",
    "    def train(self, wine_data, l_rate, batch_size=1, epochs=0, good_nb=6, bad_nb=5):\n",
    "        features, labels = select_features_labels(wine_data, good_nb, bad_nb)\n",
    "        train_until_convergence = False\n",
    "        epoch_count = 0\n",
    "        info = []\n",
    "        if (epochs == 0):\n",
    "            train_until_convergence = True # train until convergence if epochs is 0\n",
    "            epochs = 1 # epoch_count < epochs so it enters the loop\n",
    "        converged = False\n",
    "        errors_at_prev_epoch = self.test(features, labels)\n",
    "        count = 0\n",
    "        info.append((epoch_count, errors_at_prev_epoch, self.weights))\n",
    "        while (not converged and epoch_count < epochs):\n",
    "            converged = True\n",
    "            self.batch_processing(features, labels, l_rate, batch_size)\n",
    "            errors_at_epoch = self.test(features, labels)\n",
    "            if (errors_at_epoch > 0):\n",
    "                if (errors_at_epoch >= errors_at_prev_epoch):\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count = 0\n",
    "                if (count < 1000):\n",
    "                    converged = False\n",
    "            errors_at_prev_epoch = errors_at_epoch\n",
    "            epoch_count += 1\n",
    "            info.append((epoch_count, errors_at_epoch, self.weights))\n",
    "            if (train_until_convergence == True):\n",
    "                epochs = epoch_count + 1\n",
    "        return (info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Finding good hyperparameters for my ADALINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1998)\n",
    "good_threshold = 7\n",
    "bad_threshold = 4\n",
    "my_adaline = Adaline()\n",
    "perf = my_adaline.train(red_wine_df, l_rate=0.3, good_nb=good_threshold, bad_nb=bad_threshold)\n",
    "plot_perceptron_performance(red_wine_df, perf, good_threshold, bad_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1998)\n",
    "good_threshold = 6\n",
    "bad_threshold = 5\n",
    "my_adaline = Adaline()\n",
    "perf = my_adaline.train(red_wine_df, l_rate=0.03, good_nb=good_threshold, bad_nb=bad_threshold)\n",
    "plot_perceptron_performance(red_wine_df, perf, good_threshold, bad_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1998)\n",
    "good_threshold = 6\n",
    "bad_threshold = 5\n",
    "my_adaline = Adaline()\n",
    "perf = my_adaline.train(red_wine_df, l_rate=0.0003, good_nb=good_threshold, bad_nb=bad_threshold)\n",
    "plot_perceptron_performance(red_wine_df, perf, good_threshold, bad_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1998)\n",
    "good_threshold = 6\n",
    "bad_threshold = 5\n",
    "my_adaline = Adaline()\n",
    "perf = my_adaline.train(red_wine_df,\n",
    "                        l_rate=0.0002,\n",
    "                        batch_size=16,\n",
    "                        epochs=0,\n",
    "                        good_nb=good_threshold,\n",
    "                        bad_nb=bad_threshold)\n",
    "plot_perceptron_performance(red_wine_df, perf, good_threshold, bad_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.4 Advanced wine sampling and resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Holdout method to partition data in training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(wine_data, validation_split):\n",
    "    wine_data = wine_data.sample(frac=1).reset_index(drop=True) # shuffle data\n",
    "    size = len(wine_data)\n",
    "    validation_size = round(size * validation_split)\n",
    "    training_size = size - validation_size\n",
    "    return (wine_data.head(training_size), wine_data.tail(validation_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) k-fold cross-validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 2000)\n",
    "def k_fold_validation(wine_data, k, shuffle=True):\n",
    "    if (shuffle == True):\n",
    "        wine_data = wine_data.sample(frac=1).reset_index(drop=True)\n",
    "    k_size = round(len(wine_data) / k)\n",
    "    k_folds = []\n",
    "    for i in range(0, len(wine_data), k_size):\n",
    "        k_folds.append(wine_data[i:i + k_size])\n",
    "    k_tuples = []\n",
    "    for i in range(k):\n",
    "        training = pd.DataFrame()\n",
    "        for j in range(k):\n",
    "            if (i != j):\n",
    "                training = pd.concat((training, k_folds[j]))\n",
    "        k_tuples.append((training, k_folds[i]))\n",
    "    return (k_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Effects on changing learning rate and epochs: k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaline_k_fold_validation(wine_data, l_rate, k, shuffle=True,\n",
    "                              batch_size=1, epochs=0, good_nb=6, bad_nb=5):\n",
    "    if (epochs == 0):\n",
    "        return(print(\"epochs can't be 0\"))\n",
    "    tmp = wine_data[['pH', 'alcohol', 'quality']][(\n",
    "        wine_data['quality'] > good_nb) | (wine_data['quality'] < bad_nb)]\n",
    "    tmp['quality'].where(tmp['quality'] < bad_nb, 1, inplace=True) # Replace above good_quality_nb with 1\n",
    "    tmp['quality'].where(tmp['quality'] == 1, 0, inplace=True) # and below bad_quality_nb 0\n",
    "    k_folds = k_fold_validation(tmp, k, shuffle=shuffle)\n",
    "    my_adaline = Adaline()\n",
    "    \n",
    "    errors = 0\n",
    "    val_errors = 0\n",
    "    for fold in k_folds:\n",
    "        features, labels = select_features_labels(fold[0], good_nb, bad_nb)\n",
    "        val_features, val_labels = select_features_labels(fold[1], good_nb, bad_nb)\n",
    "        e = my_adaline.test(features, labels)\n",
    "        val_e = my_adaline.test(val_features, val_labels)\n",
    "        errors += e\n",
    "        val_errors += val_e\n",
    "    errors /= len(k_folds)\n",
    "    val_errors /= len(k_folds)\n",
    "    print(\"epoch 0: average training     errors: {:<4}/{:<4} => {:<2}%\".format(errors, len(labels), round(errors / len(labels) * 100, 2)))\n",
    "    print(\"         average validation   errors: {:<4}/{:<4} => {:<2}%\".format(val_errors, len(val_labels), round(val_errors / len(val_labels) * 100, 2)))\n",
    "    print(\"weights learned\", my_adaline.weights)\n",
    "    print()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        errors = 0\n",
    "        val_errors = 0\n",
    "        for fold in k_folds:\n",
    "            features, labels = select_features_labels(fold[0], good_nb, bad_nb)\n",
    "            val_features, val_labels = select_features_labels(fold[1], good_nb, bad_nb)\n",
    "            my_adaline.train(fold[0], l_rate=l_rate, batch_size=batch_size,\n",
    "                             epochs=1, good_nb=good_nb, bad_nb=bad_nb)\n",
    "            e = my_adaline.test(features, labels)\n",
    "            val_e = my_adaline.test(val_features, val_labels)\n",
    "            errors += e\n",
    "            val_errors += val_e\n",
    "        errors /= len(k_folds)\n",
    "        val_errors /= len(k_folds)\n",
    "        print(\"epoch {}: average training     errors: {:<4}/{:<4} => {:<2}%\".format(epoch + 1, errors, len(labels), round(errors / len(labels) * 100, 2)))\n",
    "        print(\"         average validation   errors: {:<4}/{:<4} => {:<2}%\".format(val_errors, len(val_labels), round(val_errors / len(val_labels) * 100, 2)))\n",
    "        print(\"weights learned\", my_adaline.weights)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "adaline_k_fold_validation(red_wine_df, l_rate=0.003, k=10, epochs=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "adaline_k_fold_validation(red_wine_df, l_rate=0.003, k=10, epochs=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "adaline_k_fold_validation(red_wine_df, l_rate=0.00003, k=10, epochs=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.5 Adventures in the Nth dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_labels_2(wine_data, feature_list, good_quality_nb, bad_quality_nb):\n",
    "    tmp = wine_data[feature_list + ['quality']][(\n",
    "        wine_data['quality'] > good_quality_nb) | (wine_data['quality'] < bad_quality_nb)]\n",
    "    tmp['quality'].where(tmp['quality'] < bad_quality_nb, 1, inplace=True) # Replace good_quality_nb and above with 1\n",
    "    tmp['quality'].where(tmp['quality'] == 1, 0, inplace=True) # and bad quality and below with 0\n",
    "    features = tmp[feature_list]\n",
    "    labels = tmp[['quality']]\n",
    "    features = [list(features[item]) for item in features]\n",
    "    labels = [list(labels[item]) for item in labels]\n",
    "    labels = labels[0]\n",
    "    return (features, labels) # returns a list of lists containing the values of each feature\n",
    "\n",
    "class Adaline_2:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = [0] * (input_size + 1)\n",
    "        for i in range(input_size + 1):\n",
    "            self.weights[i] = round(random.uniform(-1, 1), 2) # w[0] is the bias\n",
    "    def test(self, features, labels):\n",
    "        error_count = 0\n",
    "        for x1, x2, x3, y in zip(features[0], features[1], features[2], labels):\n",
    "            x = [1, x1, x2, x3]\n",
    "            if (dot_prod(self.weights, x) >= 0.5):\n",
    "                output = 1\n",
    "            else:\n",
    "                output = 0\n",
    "            if (y != output):\n",
    "                error_count += 1\n",
    "        return (error_count)\n",
    "    def next_batch(self, features, labels, batch_size):\n",
    "        for i in range(0, len(labels), batch_size):\n",
    "            yield features[0][i:i + batch_size], features[1][i:i + batch_size], features[2][i:i + batch_size], labels[i:i + batch_size]\n",
    "    def batch_processing(self, features, labels, l_rate, batch_size):\n",
    "        generator = self.next_batch(features, labels, batch_size)\n",
    "        for batch_x1, batch_x2, batch_x3, batch_y in generator:\n",
    "            batch_error = 0\n",
    "            batch_gradient = [0, 0, 0, 0]\n",
    "            for x1, x2, x3, y in zip(batch_x1, batch_x2, batch_x3, batch_y):\n",
    "                x = [1, x1, x2, x3]\n",
    "                error = y - dot_prod(self.weights, x)\n",
    "                gradient = vec_scale(v=x, scalar=2 * l_rate * error)\n",
    "                batch_error += error\n",
    "                batch_gradient = vec_add(batch_gradient, gradient)\n",
    "            batch_error /= len(batch_x1)\n",
    "            self.weights = vec_add(self.weights, batch_gradient)\n",
    "    def train(self, wine_data, feature_list, l_rate, batch_size=1, epochs=0, good_nb=6, bad_nb=5):\n",
    "        features, labels = select_features_labels_2(wine_data, feature_list, good_nb, bad_nb)\n",
    "        train_until_convergence = False\n",
    "        epoch_count = 0\n",
    "        info = []\n",
    "        if (epochs == 0):\n",
    "            train_until_convergence = True # train until convergence if epochs is 0\n",
    "            epochs = 1 # epoch_count < epochs so it enters the loop\n",
    "        converged = False\n",
    "        errors_at_prev_epoch = self.test(features, labels)\n",
    "        count = 0\n",
    "        print(\"epoch {}: total errors: {:<4}/{:<4} => {:<2}%\".format(epoch_count, errors_at_prev_epoch, len(labels), round(errors_at_prev_epoch / len(labels) * 100, 2)))\n",
    "        print(\"weights learned\", my_adaline.weights)\n",
    "        info.append((epoch_count, errors_at_prev_epoch, self.weights))\n",
    "        while (not converged and epoch_count < epochs):\n",
    "            converged = True\n",
    "            self.batch_processing(features, labels, l_rate, batch_size)\n",
    "            errors_at_epoch = self.test(features, labels)\n",
    "            if (errors_at_epoch > 0):\n",
    "                if (errors_at_epoch >= errors_at_prev_epoch):\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count = 0\n",
    "                if (count < 1000):\n",
    "                    converged = False\n",
    "            errors_at_prev_epoch = errors_at_epoch\n",
    "            epoch_count += 1\n",
    "            print(\"epoch {}: total errors: {:<4}/{:<4} => {:<2}%\".format(epoch_count, errors_at_epoch, len(labels), round(errors_at_epoch / len(labels) * 100, 2)))\n",
    "            print(\"weights learned\", my_adaline.weights)\n",
    "            info.append((epoch_count, errors_at_epoch, self.weights))\n",
    "            if (train_until_convergence == True):\n",
    "                epochs = epoch_count + 1\n",
    "        return (info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "feature_list = [\"pH\", \"alcohol\", \"sulphates\"]\n",
    "my_adaline = Adaline_2(input_size=3)\n",
    "perf = my_adaline.train(red_wine_df, feature_list, l_rate=0.0001, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline_3:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = [0] * (input_size + 1)\n",
    "        for i in range(input_size + 1):\n",
    "            self.weights[i] = round(random.uniform(-1, 1), 2) # w[0] is the bias\n",
    "    def test(self, features, labels):\n",
    "        error_count = 0\n",
    "        for x1, x2, x3, x4, y in zip(features[0], features[1], features[2], features[3], labels):\n",
    "            x = [1, x1, x2, x3, x4]\n",
    "            if (dot_prod(self.weights, x) >= 0.5):\n",
    "                output = 1\n",
    "            else:\n",
    "                output = 0\n",
    "            if (y != output):\n",
    "                error_count += 1\n",
    "        return (error_count)\n",
    "    def next_batch(self, features, labels, batch_size):\n",
    "        for i in range(0, len(labels), batch_size):\n",
    "            yield features[0][i:i + batch_size], features[1][i:i + batch_size], features[2][i:i + batch_size], features[3][i:i + batch_size], labels[i:i + batch_size]\n",
    "    def batch_processing(self, features, labels, l_rate, batch_size):\n",
    "        generator = self.next_batch(features, labels, batch_size)\n",
    "        for batch_x1, batch_x2, batch_x3, batch_x4, batch_y in generator:\n",
    "            batch_error = 0\n",
    "            batch_gradient = [0, 0, 0, 0, 0]\n",
    "            for x1, x2, x3, x4, y in zip(batch_x1, batch_x2, batch_x3, batch_x4, batch_y):\n",
    "                x = [1, x1, x2, x3, x4]\n",
    "                error = y - dot_prod(self.weights, x)\n",
    "                gradient = vec_scale(v=x, scalar=2 * l_rate * error)\n",
    "                batch_error += error\n",
    "                batch_gradient = vec_add(batch_gradient, gradient)\n",
    "            batch_error /= len(batch_x1)\n",
    "            self.weights = vec_add(self.weights, batch_gradient)\n",
    "    def train(self, wine_data, feature_list, l_rate, batch_size=1, epochs=0, good_nb=6, bad_nb=5):\n",
    "        features, labels = select_features_labels_2(wine_data, feature_list, good_nb, bad_nb)\n",
    "        train_until_convergence = False\n",
    "        epoch_count = 0\n",
    "        info = []\n",
    "        if (epochs == 0):\n",
    "            train_until_convergence = True # train until convergence if epochs is 0\n",
    "            epochs = 1 # epoch_count < epochs so it enters the loop\n",
    "        converged = False\n",
    "        errors_at_prev_epoch = self.test(features, labels)\n",
    "        count = 0\n",
    "        print(\"epoch {}: total errors: {:<4}/{:<4} => {:<2}%\".format(epoch_count, errors_at_prev_epoch, len(labels), round(errors_at_prev_epoch / len(labels) * 100, 2)))\n",
    "        print(\"weights learned\", my_adaline.weights)\n",
    "        info.append((epoch_count, errors_at_prev_epoch, self.weights))\n",
    "        while (not converged and epoch_count < epochs):\n",
    "            converged = True\n",
    "            self.batch_processing(features, labels, l_rate, batch_size)\n",
    "            errors_at_epoch = self.test(features, labels)\n",
    "            if (errors_at_epoch > 0):\n",
    "                if (errors_at_epoch >= errors_at_prev_epoch):\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count = 0\n",
    "                if (count < 1000):\n",
    "                    converged = False\n",
    "            errors_at_prev_epoch = errors_at_epoch\n",
    "            epoch_count += 1\n",
    "            print(\"epoch {}: total errors: {:<4}/{:<4} => {:<2}%\".format(epoch_count, errors_at_epoch, len(labels), round(errors_at_epoch / len(labels) * 100, 2)))\n",
    "            print(\"weights learned\", my_adaline.weights)\n",
    "            info.append((epoch_count, errors_at_epoch, self.weights))\n",
    "            if (train_until_convergence == True):\n",
    "                epochs = epoch_count + 1\n",
    "        return (info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "feature_list = [\"pH\", \"alcohol\", \"sulphates\", \"chlorides\"]\n",
    "my_adaline = Adaline_3(input_size=4)\n",
    "perf = my_adaline.train(red_wine_df, feature_list, l_rate=0.0001, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Multiple dimensions decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of 3 dimensions the decision boundary will be a plane that separates the data in a 3d space. In 4 or higher dimensions it will be a hyperplane and we can't visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.6 Marvin's rebuttal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Pan-Galactic Gargle Blaster dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galactic_df = pd.read_csv(\"resources/Pan Galactic Gargle Blaster.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galactic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galactic_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_matrix(galactic_df, 7, 4, rows=3, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just from looking at the data we can clearly see that it's impossible to classify it with a perceptron or Adaline because this algorithms are linear. There is no possible line that could separate this dataset. We need to use more complex nerworks to solve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline_galactic:\n",
    "    def __init__(self):\n",
    "        self.weights = [0, 0, 0]\n",
    "        for i in range(3):\n",
    "            self.weights[i] = round(random.uniform(-1, 1), 2) # w[0] is the bias\n",
    "    def test(self, features, labels):\n",
    "        error_count = 0\n",
    "        for x1, x2, y in zip(features[0], features[1], labels):\n",
    "            x = [1, x1, x2]\n",
    "            if (dot_prod(self.weights, x) >= 0.5):\n",
    "                output = 1\n",
    "            else:\n",
    "                output = 0\n",
    "            if (y != output):\n",
    "                error_count += 1\n",
    "        return (error_count)\n",
    "    def next_batch(self, features, labels, batch_size):\n",
    "        for i in range(0, len(labels), batch_size):\n",
    "            yield features[0][i:i + batch_size], features[1][i:i + batch_size], labels[i:i + batch_size]\n",
    "    def batch_processing(self, features, labels, l_rate, batch_size):\n",
    "        generator = self.next_batch(features, labels, batch_size)\n",
    "        for batch_x1, batch_x2, batch_y in generator:\n",
    "            batch_error = 0\n",
    "            batch_gradient = [0, 0, 0]\n",
    "            for x1, x2, y in zip(batch_x1, batch_x2, batch_y):\n",
    "                x = [1, x1, x2]\n",
    "                error = y - dot_prod(self.weights, x)\n",
    "                gradient = vec_scale(v=x, scalar=2 * l_rate * error)\n",
    "                batch_error += error\n",
    "                batch_gradient = vec_add(batch_gradient, gradient)\n",
    "            batch_error /= len(batch_x1)\n",
    "            self.weights = vec_add(self.weights, batch_gradient)\n",
    "    def train(self, data, l_rate, batch_size=1, epochs=0, good_nb=6, bad_nb=5):\n",
    "        features, labels = select_features_labels(data, good_nb, bad_nb, features_list=[\"wonderflonium\", \"fallian marsh gas\"])\n",
    "        train_until_convergence = False\n",
    "        epoch_count = 0\n",
    "        info = []\n",
    "        if (epochs == 0):\n",
    "            train_until_convergence = True # train until convergence if epochs is 0\n",
    "            epochs = 1 # epoch_count < epochs so it enters the loop\n",
    "        converged = False\n",
    "        errors_at_prev_epoch = self.test(features, labels)\n",
    "        count = 0\n",
    "        info.append((epoch_count, errors_at_prev_epoch, self.weights))\n",
    "        while (not converged and epoch_count < epochs):\n",
    "            converged = True\n",
    "            self.batch_processing(features, labels, l_rate, batch_size)\n",
    "            errors_at_epoch = self.test(features, labels)\n",
    "            if (errors_at_epoch > 0):\n",
    "                if (errors_at_epoch >= errors_at_prev_epoch):\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count = 0\n",
    "                if (count < 1000):\n",
    "                    converged = False\n",
    "            errors_at_prev_epoch = errors_at_epoch\n",
    "            epoch_count += 1\n",
    "            info.append((epoch_count, errors_at_epoch, self.weights))\n",
    "            if (train_until_convergence == True):\n",
    "                epochs = epoch_count + 1\n",
    "        return (info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "good_threshold = 7\n",
    "bad_threshold = 4\n",
    "my_adaline = Adaline_galactic()\n",
    "perf = my_adaline.train(galactic_df, l_rate=0.003, good_nb=good_threshold, bad_nb=bad_threshold)\n",
    "plot_perceptron_performance(galactic_df, perf, good_threshold, bad_threshold, feature_list=[\"wonderflonium\", \"fallian marsh gas\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
